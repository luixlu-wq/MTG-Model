{
  "best_global_step": 200,
  "best_metric": 0.1744702309370041,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 100,
  "global_step": 254,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007889546351084813,
      "grad_norm": 0.2986903488636017,
      "learning_rate": 0.0001,
      "loss": 1.2756377458572388,
      "step": 1
    },
    {
      "epoch": 0.015779092702169626,
      "grad_norm": 0.32277196645736694,
      "learning_rate": 9.960629921259843e-05,
      "loss": 1.2386047840118408,
      "step": 2
    },
    {
      "epoch": 0.023668639053254437,
      "grad_norm": 0.3710992634296417,
      "learning_rate": 9.921259842519686e-05,
      "loss": 1.1541543006896973,
      "step": 3
    },
    {
      "epoch": 0.03155818540433925,
      "grad_norm": 0.2865287959575653,
      "learning_rate": 9.881889763779529e-05,
      "loss": 1.0805875062942505,
      "step": 4
    },
    {
      "epoch": 0.03944773175542406,
      "grad_norm": 0.28394651412963867,
      "learning_rate": 9.842519685039371e-05,
      "loss": 1.019783854484558,
      "step": 5
    },
    {
      "epoch": 0.047337278106508875,
      "grad_norm": 0.2733297348022461,
      "learning_rate": 9.803149606299214e-05,
      "loss": 0.9728843569755554,
      "step": 6
    },
    {
      "epoch": 0.055226824457593686,
      "grad_norm": 0.2589825391769409,
      "learning_rate": 9.763779527559055e-05,
      "loss": 0.9514358639717102,
      "step": 7
    },
    {
      "epoch": 0.0631163708086785,
      "grad_norm": 0.29216310381889343,
      "learning_rate": 9.724409448818898e-05,
      "loss": 0.9349378943443298,
      "step": 8
    },
    {
      "epoch": 0.07100591715976332,
      "grad_norm": 0.265587717294693,
      "learning_rate": 9.68503937007874e-05,
      "loss": 0.9827988147735596,
      "step": 9
    },
    {
      "epoch": 0.07889546351084813,
      "grad_norm": 0.22090809047222137,
      "learning_rate": 9.645669291338582e-05,
      "loss": 0.9046621322631836,
      "step": 10
    },
    {
      "epoch": 0.08678500986193294,
      "grad_norm": 0.2124425768852234,
      "learning_rate": 9.606299212598425e-05,
      "loss": 0.8590143322944641,
      "step": 11
    },
    {
      "epoch": 0.09467455621301775,
      "grad_norm": 0.20316185057163239,
      "learning_rate": 9.566929133858268e-05,
      "loss": 0.7849445939064026,
      "step": 12
    },
    {
      "epoch": 0.10256410256410256,
      "grad_norm": 0.21276457607746124,
      "learning_rate": 9.52755905511811e-05,
      "loss": 0.8387272953987122,
      "step": 13
    },
    {
      "epoch": 0.11045364891518737,
      "grad_norm": 0.210178405046463,
      "learning_rate": 9.488188976377953e-05,
      "loss": 0.7285286784172058,
      "step": 14
    },
    {
      "epoch": 0.11834319526627218,
      "grad_norm": 1.3945482969284058,
      "learning_rate": 9.448818897637796e-05,
      "loss": 0.7126011252403259,
      "step": 15
    },
    {
      "epoch": 0.126232741617357,
      "grad_norm": 0.23730292916297913,
      "learning_rate": 9.409448818897638e-05,
      "loss": 0.7777173519134521,
      "step": 16
    },
    {
      "epoch": 0.1341222879684418,
      "grad_norm": 0.22973985970020294,
      "learning_rate": 9.370078740157481e-05,
      "loss": 0.7548772096633911,
      "step": 17
    },
    {
      "epoch": 0.14201183431952663,
      "grad_norm": 0.2520158588886261,
      "learning_rate": 9.330708661417324e-05,
      "loss": 0.7277920842170715,
      "step": 18
    },
    {
      "epoch": 0.14990138067061143,
      "grad_norm": 0.2524684965610504,
      "learning_rate": 9.291338582677166e-05,
      "loss": 0.7120368480682373,
      "step": 19
    },
    {
      "epoch": 0.15779092702169625,
      "grad_norm": 0.220575749874115,
      "learning_rate": 9.251968503937009e-05,
      "loss": 0.6408388018608093,
      "step": 20
    },
    {
      "epoch": 0.16568047337278108,
      "grad_norm": 0.30192917585372925,
      "learning_rate": 9.21259842519685e-05,
      "loss": 0.6250965595245361,
      "step": 21
    },
    {
      "epoch": 0.17357001972386588,
      "grad_norm": 0.251997172832489,
      "learning_rate": 9.173228346456693e-05,
      "loss": 0.7111013531684875,
      "step": 22
    },
    {
      "epoch": 0.1814595660749507,
      "grad_norm": 0.3329823613166809,
      "learning_rate": 9.133858267716536e-05,
      "loss": 0.5616381168365479,
      "step": 23
    },
    {
      "epoch": 0.1893491124260355,
      "grad_norm": 0.4002377390861511,
      "learning_rate": 9.094488188976379e-05,
      "loss": 0.5809764266014099,
      "step": 24
    },
    {
      "epoch": 0.19723865877712032,
      "grad_norm": 0.36111098527908325,
      "learning_rate": 9.05511811023622e-05,
      "loss": 0.4961633086204529,
      "step": 25
    },
    {
      "epoch": 0.20512820512820512,
      "grad_norm": 0.28819966316223145,
      "learning_rate": 9.015748031496063e-05,
      "loss": 0.5136879086494446,
      "step": 26
    },
    {
      "epoch": 0.21301775147928995,
      "grad_norm": 0.31631895899772644,
      "learning_rate": 8.976377952755905e-05,
      "loss": 0.4814431965351105,
      "step": 27
    },
    {
      "epoch": 0.22090729783037474,
      "grad_norm": 0.35535532236099243,
      "learning_rate": 8.937007874015748e-05,
      "loss": 0.46876949071884155,
      "step": 28
    },
    {
      "epoch": 0.22879684418145957,
      "grad_norm": 0.42927783727645874,
      "learning_rate": 8.897637795275591e-05,
      "loss": 0.37179985642433167,
      "step": 29
    },
    {
      "epoch": 0.23668639053254437,
      "grad_norm": 0.37909185886383057,
      "learning_rate": 8.858267716535433e-05,
      "loss": 0.36148956418037415,
      "step": 30
    },
    {
      "epoch": 0.2445759368836292,
      "grad_norm": 0.3141479194164276,
      "learning_rate": 8.818897637795276e-05,
      "loss": 0.3788332939147949,
      "step": 31
    },
    {
      "epoch": 0.252465483234714,
      "grad_norm": 0.3333270847797394,
      "learning_rate": 8.779527559055119e-05,
      "loss": 0.2911955416202545,
      "step": 32
    },
    {
      "epoch": 0.2603550295857988,
      "grad_norm": 0.29804638028144836,
      "learning_rate": 8.740157480314962e-05,
      "loss": 0.31011825799942017,
      "step": 33
    },
    {
      "epoch": 0.2682445759368836,
      "grad_norm": 0.23957665264606476,
      "learning_rate": 8.700787401574804e-05,
      "loss": 0.2787698209285736,
      "step": 34
    },
    {
      "epoch": 0.27613412228796846,
      "grad_norm": 0.23877152800559998,
      "learning_rate": 8.661417322834646e-05,
      "loss": 0.2770247757434845,
      "step": 35
    },
    {
      "epoch": 0.28402366863905326,
      "grad_norm": 0.3390362858772278,
      "learning_rate": 8.622047244094488e-05,
      "loss": 0.4088314473628998,
      "step": 36
    },
    {
      "epoch": 0.29191321499013806,
      "grad_norm": 0.2721662223339081,
      "learning_rate": 8.582677165354331e-05,
      "loss": 0.28800472617149353,
      "step": 37
    },
    {
      "epoch": 0.29980276134122286,
      "grad_norm": 0.293945848941803,
      "learning_rate": 8.543307086614174e-05,
      "loss": 0.22085678577423096,
      "step": 38
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 0.2193019688129425,
      "learning_rate": 8.503937007874016e-05,
      "loss": 0.36652103066444397,
      "step": 39
    },
    {
      "epoch": 0.3155818540433925,
      "grad_norm": 0.19913245737552643,
      "learning_rate": 8.464566929133859e-05,
      "loss": 0.23225028812885284,
      "step": 40
    },
    {
      "epoch": 0.3234714003944773,
      "grad_norm": 0.25689172744750977,
      "learning_rate": 8.4251968503937e-05,
      "loss": 0.2800310552120209,
      "step": 41
    },
    {
      "epoch": 0.33136094674556216,
      "grad_norm": 0.15649870038032532,
      "learning_rate": 8.385826771653543e-05,
      "loss": 0.19853539764881134,
      "step": 42
    },
    {
      "epoch": 0.33925049309664695,
      "grad_norm": 0.1464175134897232,
      "learning_rate": 8.346456692913386e-05,
      "loss": 0.18909822404384613,
      "step": 43
    },
    {
      "epoch": 0.34714003944773175,
      "grad_norm": 0.2116643488407135,
      "learning_rate": 8.307086614173229e-05,
      "loss": 0.2891447842121124,
      "step": 44
    },
    {
      "epoch": 0.35502958579881655,
      "grad_norm": 0.1704811006784439,
      "learning_rate": 8.267716535433071e-05,
      "loss": 0.6687749028205872,
      "step": 45
    },
    {
      "epoch": 0.3629191321499014,
      "grad_norm": 0.16545920073986053,
      "learning_rate": 8.228346456692914e-05,
      "loss": 0.2483631670475006,
      "step": 46
    },
    {
      "epoch": 0.3708086785009862,
      "grad_norm": 0.18452638387680054,
      "learning_rate": 8.188976377952757e-05,
      "loss": 0.22171497344970703,
      "step": 47
    },
    {
      "epoch": 0.378698224852071,
      "grad_norm": 0.18238221108913422,
      "learning_rate": 8.149606299212598e-05,
      "loss": 0.2611050307750702,
      "step": 48
    },
    {
      "epoch": 0.3865877712031558,
      "grad_norm": 0.15663817524909973,
      "learning_rate": 8.110236220472441e-05,
      "loss": 0.262088418006897,
      "step": 49
    },
    {
      "epoch": 0.39447731755424065,
      "grad_norm": 0.18556109070777893,
      "learning_rate": 8.070866141732284e-05,
      "loss": 0.22245261073112488,
      "step": 50
    },
    {
      "epoch": 0.40236686390532544,
      "grad_norm": 0.1613033562898636,
      "learning_rate": 8.031496062992126e-05,
      "loss": 0.19412384927272797,
      "step": 51
    },
    {
      "epoch": 0.41025641025641024,
      "grad_norm": 0.13100366294384003,
      "learning_rate": 7.992125984251969e-05,
      "loss": 0.1829841583967209,
      "step": 52
    },
    {
      "epoch": 0.4181459566074951,
      "grad_norm": 0.15873824059963226,
      "learning_rate": 7.952755905511812e-05,
      "loss": 0.27675414085388184,
      "step": 53
    },
    {
      "epoch": 0.4260355029585799,
      "grad_norm": 0.17513523995876312,
      "learning_rate": 7.913385826771654e-05,
      "loss": 0.7002593278884888,
      "step": 54
    },
    {
      "epoch": 0.4339250493096647,
      "grad_norm": 0.13758674263954163,
      "learning_rate": 7.874015748031497e-05,
      "loss": 0.24822644889354706,
      "step": 55
    },
    {
      "epoch": 0.4418145956607495,
      "grad_norm": 0.15246887505054474,
      "learning_rate": 7.83464566929134e-05,
      "loss": 0.2632077634334564,
      "step": 56
    },
    {
      "epoch": 0.44970414201183434,
      "grad_norm": 0.17593172192573547,
      "learning_rate": 7.795275590551181e-05,
      "loss": 0.16555571556091309,
      "step": 57
    },
    {
      "epoch": 0.45759368836291914,
      "grad_norm": 0.2102516144514084,
      "learning_rate": 7.755905511811024e-05,
      "loss": 0.19077441096305847,
      "step": 58
    },
    {
      "epoch": 0.46548323471400394,
      "grad_norm": 0.15275035798549652,
      "learning_rate": 7.716535433070867e-05,
      "loss": 0.6961777806282043,
      "step": 59
    },
    {
      "epoch": 0.47337278106508873,
      "grad_norm": 0.1622527688741684,
      "learning_rate": 7.677165354330709e-05,
      "loss": 0.1935388445854187,
      "step": 60
    },
    {
      "epoch": 0.4812623274161736,
      "grad_norm": 0.19804665446281433,
      "learning_rate": 7.637795275590552e-05,
      "loss": 0.16568507254123688,
      "step": 61
    },
    {
      "epoch": 0.4891518737672584,
      "grad_norm": 0.16866028308868408,
      "learning_rate": 7.598425196850393e-05,
      "loss": 0.19573277235031128,
      "step": 62
    },
    {
      "epoch": 0.4970414201183432,
      "grad_norm": 0.14098121225833893,
      "learning_rate": 7.559055118110236e-05,
      "loss": 0.20979414880275726,
      "step": 63
    },
    {
      "epoch": 0.504930966469428,
      "grad_norm": 0.16365504264831543,
      "learning_rate": 7.519685039370079e-05,
      "loss": 0.21958760917186737,
      "step": 64
    },
    {
      "epoch": 0.5128205128205128,
      "grad_norm": 0.1792217344045639,
      "learning_rate": 7.480314960629921e-05,
      "loss": 0.22656774520874023,
      "step": 65
    },
    {
      "epoch": 0.5207100591715976,
      "grad_norm": 0.22601358592510223,
      "learning_rate": 7.440944881889764e-05,
      "loss": 0.2475948929786682,
      "step": 66
    },
    {
      "epoch": 0.5285996055226825,
      "grad_norm": 0.20559747517108917,
      "learning_rate": 7.401574803149607e-05,
      "loss": 0.22951461374759674,
      "step": 67
    },
    {
      "epoch": 0.5364891518737672,
      "grad_norm": 0.18001553416252136,
      "learning_rate": 7.36220472440945e-05,
      "loss": 0.5167295336723328,
      "step": 68
    },
    {
      "epoch": 0.5443786982248521,
      "grad_norm": 0.13399212062358856,
      "learning_rate": 7.322834645669292e-05,
      "loss": 0.2131829410791397,
      "step": 69
    },
    {
      "epoch": 0.5522682445759369,
      "grad_norm": 0.14544114470481873,
      "learning_rate": 7.283464566929135e-05,
      "loss": 0.17796850204467773,
      "step": 70
    },
    {
      "epoch": 0.5601577909270217,
      "grad_norm": 0.15248623490333557,
      "learning_rate": 7.244094488188978e-05,
      "loss": 0.22071640193462372,
      "step": 71
    },
    {
      "epoch": 0.5680473372781065,
      "grad_norm": 0.13172051310539246,
      "learning_rate": 7.20472440944882e-05,
      "loss": 0.19264452159404755,
      "step": 72
    },
    {
      "epoch": 0.5759368836291914,
      "grad_norm": 0.14625751972198486,
      "learning_rate": 7.165354330708662e-05,
      "loss": 0.21994420886039734,
      "step": 73
    },
    {
      "epoch": 0.5838264299802761,
      "grad_norm": 0.1508413553237915,
      "learning_rate": 7.125984251968504e-05,
      "loss": 0.22089703381061554,
      "step": 74
    },
    {
      "epoch": 0.591715976331361,
      "grad_norm": 0.14263282716274261,
      "learning_rate": 7.086614173228347e-05,
      "loss": 0.21031250059604645,
      "step": 75
    },
    {
      "epoch": 0.5996055226824457,
      "grad_norm": 0.1771865040063858,
      "learning_rate": 7.047244094488188e-05,
      "loss": 0.17909370362758636,
      "step": 76
    },
    {
      "epoch": 0.6074950690335306,
      "grad_norm": 0.16332605481147766,
      "learning_rate": 7.007874015748031e-05,
      "loss": 0.1970411241054535,
      "step": 77
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 0.1470152884721756,
      "learning_rate": 6.968503937007874e-05,
      "loss": 0.19781652092933655,
      "step": 78
    },
    {
      "epoch": 0.6232741617357002,
      "grad_norm": 0.16089408099651337,
      "learning_rate": 6.929133858267717e-05,
      "loss": 0.19839303195476532,
      "step": 79
    },
    {
      "epoch": 0.631163708086785,
      "grad_norm": 0.12510034441947937,
      "learning_rate": 6.889763779527559e-05,
      "loss": 0.1773466169834137,
      "step": 80
    },
    {
      "epoch": 0.6390532544378699,
      "grad_norm": 0.13853400945663452,
      "learning_rate": 6.850393700787402e-05,
      "loss": 0.20971986651420593,
      "step": 81
    },
    {
      "epoch": 0.6469428007889546,
      "grad_norm": 0.15086062252521515,
      "learning_rate": 6.811023622047245e-05,
      "loss": 0.20640043914318085,
      "step": 82
    },
    {
      "epoch": 0.6548323471400395,
      "grad_norm": 0.1423478126525879,
      "learning_rate": 6.771653543307087e-05,
      "loss": 0.1436776965856552,
      "step": 83
    },
    {
      "epoch": 0.6627218934911243,
      "grad_norm": 0.15978866815567017,
      "learning_rate": 6.73228346456693e-05,
      "loss": 0.2118891328573227,
      "step": 84
    },
    {
      "epoch": 0.6706114398422091,
      "grad_norm": 0.19469256699085236,
      "learning_rate": 6.692913385826773e-05,
      "loss": 0.22638466954231262,
      "step": 85
    },
    {
      "epoch": 0.6785009861932939,
      "grad_norm": 0.14751401543617249,
      "learning_rate": 6.653543307086615e-05,
      "loss": 0.22019857168197632,
      "step": 86
    },
    {
      "epoch": 0.6863905325443787,
      "grad_norm": 0.20659086108207703,
      "learning_rate": 6.614173228346457e-05,
      "loss": 0.24545405805110931,
      "step": 87
    },
    {
      "epoch": 0.6942800788954635,
      "grad_norm": 0.16114051640033722,
      "learning_rate": 6.5748031496063e-05,
      "loss": 0.3321170210838318,
      "step": 88
    },
    {
      "epoch": 0.7021696252465484,
      "grad_norm": 0.1366787999868393,
      "learning_rate": 6.535433070866141e-05,
      "loss": 0.1941721886396408,
      "step": 89
    },
    {
      "epoch": 0.7100591715976331,
      "grad_norm": 0.15422703325748444,
      "learning_rate": 6.496062992125984e-05,
      "loss": 0.22165100276470184,
      "step": 90
    },
    {
      "epoch": 0.717948717948718,
      "grad_norm": 0.20869259536266327,
      "learning_rate": 6.456692913385826e-05,
      "loss": 0.20169924199581146,
      "step": 91
    },
    {
      "epoch": 0.7258382642998028,
      "grad_norm": 0.15821219980716705,
      "learning_rate": 6.417322834645669e-05,
      "loss": 0.24041512608528137,
      "step": 92
    },
    {
      "epoch": 0.7337278106508875,
      "grad_norm": 0.13622315227985382,
      "learning_rate": 6.377952755905512e-05,
      "loss": 0.19122770428657532,
      "step": 93
    },
    {
      "epoch": 0.7416173570019724,
      "grad_norm": 0.15581709146499634,
      "learning_rate": 6.338582677165354e-05,
      "loss": 0.29833292961120605,
      "step": 94
    },
    {
      "epoch": 0.7495069033530573,
      "grad_norm": 0.16178861260414124,
      "learning_rate": 6.299212598425197e-05,
      "loss": 0.20558077096939087,
      "step": 95
    },
    {
      "epoch": 0.757396449704142,
      "grad_norm": 0.18363001942634583,
      "learning_rate": 6.25984251968504e-05,
      "loss": 0.29694071412086487,
      "step": 96
    },
    {
      "epoch": 0.7652859960552268,
      "grad_norm": 0.15035158395767212,
      "learning_rate": 6.220472440944882e-05,
      "loss": 0.15443554520606995,
      "step": 97
    },
    {
      "epoch": 0.7731755424063116,
      "grad_norm": 0.17804665863513947,
      "learning_rate": 6.181102362204725e-05,
      "loss": 0.2456578016281128,
      "step": 98
    },
    {
      "epoch": 0.7810650887573964,
      "grad_norm": 0.13983963429927826,
      "learning_rate": 6.141732283464568e-05,
      "loss": 0.18543177843093872,
      "step": 99
    },
    {
      "epoch": 0.7889546351084813,
      "grad_norm": 0.15058396756649017,
      "learning_rate": 6.10236220472441e-05,
      "loss": 0.24394041299819946,
      "step": 100
    },
    {
      "epoch": 0.7889546351084813,
      "eval_loss": 0.20167143642902374,
      "eval_runtime": 35.607,
      "eval_samples_per_second": 1.517,
      "eval_steps_per_second": 0.197,
      "step": 100
    },
    {
      "epoch": 0.796844181459566,
      "grad_norm": 0.1694926768541336,
      "learning_rate": 6.0629921259842526e-05,
      "loss": 0.4462635815143585,
      "step": 101
    },
    {
      "epoch": 0.8047337278106509,
      "grad_norm": 0.14634577929973602,
      "learning_rate": 6.0236220472440953e-05,
      "loss": 0.1919962465763092,
      "step": 102
    },
    {
      "epoch": 0.8126232741617357,
      "grad_norm": 0.14618195593357086,
      "learning_rate": 5.984251968503938e-05,
      "loss": 0.1665070652961731,
      "step": 103
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 0.150944322347641,
      "learning_rate": 5.94488188976378e-05,
      "loss": 0.21162919700145721,
      "step": 104
    },
    {
      "epoch": 0.8284023668639053,
      "grad_norm": 0.15662124752998352,
      "learning_rate": 5.905511811023622e-05,
      "loss": 0.17995649576187134,
      "step": 105
    },
    {
      "epoch": 0.8362919132149902,
      "grad_norm": 0.2772742211818695,
      "learning_rate": 5.866141732283464e-05,
      "loss": 0.16529278457164764,
      "step": 106
    },
    {
      "epoch": 0.8441814595660749,
      "grad_norm": 0.16106250882148743,
      "learning_rate": 5.826771653543307e-05,
      "loss": 0.21055231988430023,
      "step": 107
    },
    {
      "epoch": 0.8520710059171598,
      "grad_norm": 0.13721519708633423,
      "learning_rate": 5.7874015748031495e-05,
      "loss": 0.17958292365074158,
      "step": 108
    },
    {
      "epoch": 0.8599605522682445,
      "grad_norm": 0.15663067996501923,
      "learning_rate": 5.748031496062992e-05,
      "loss": 0.2825150489807129,
      "step": 109
    },
    {
      "epoch": 0.8678500986193294,
      "grad_norm": 0.14766444265842438,
      "learning_rate": 5.708661417322835e-05,
      "loss": 0.21055394411087036,
      "step": 110
    },
    {
      "epoch": 0.8757396449704142,
      "grad_norm": 0.13522179424762726,
      "learning_rate": 5.6692913385826777e-05,
      "loss": 0.16255466639995575,
      "step": 111
    },
    {
      "epoch": 0.883629191321499,
      "grad_norm": 0.15703913569450378,
      "learning_rate": 5.62992125984252e-05,
      "loss": 0.1713373064994812,
      "step": 112
    },
    {
      "epoch": 0.8915187376725838,
      "grad_norm": 0.15733060240745544,
      "learning_rate": 5.5905511811023624e-05,
      "loss": 0.2107161283493042,
      "step": 113
    },
    {
      "epoch": 0.8994082840236687,
      "grad_norm": 0.14556492865085602,
      "learning_rate": 5.551181102362205e-05,
      "loss": 0.2085755169391632,
      "step": 114
    },
    {
      "epoch": 0.9072978303747534,
      "grad_norm": 0.19667750597000122,
      "learning_rate": 5.511811023622048e-05,
      "loss": 0.2686804234981537,
      "step": 115
    },
    {
      "epoch": 0.9151873767258383,
      "grad_norm": 0.16400843858718872,
      "learning_rate": 5.4724409448818905e-05,
      "loss": 0.18365010619163513,
      "step": 116
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 0.1490950584411621,
      "learning_rate": 5.433070866141733e-05,
      "loss": 0.16914314031600952,
      "step": 117
    },
    {
      "epoch": 0.9309664694280079,
      "grad_norm": 0.1779964566230774,
      "learning_rate": 5.393700787401575e-05,
      "loss": 0.2521914541721344,
      "step": 118
    },
    {
      "epoch": 0.9388560157790927,
      "grad_norm": 0.17349044978618622,
      "learning_rate": 5.354330708661418e-05,
      "loss": 0.1984872817993164,
      "step": 119
    },
    {
      "epoch": 0.9467455621301775,
      "grad_norm": 0.134846493601799,
      "learning_rate": 5.3149606299212606e-05,
      "loss": 0.1681501567363739,
      "step": 120
    },
    {
      "epoch": 0.9546351084812623,
      "grad_norm": 0.14450626075267792,
      "learning_rate": 5.275590551181102e-05,
      "loss": 0.22659572958946228,
      "step": 121
    },
    {
      "epoch": 0.9625246548323472,
      "grad_norm": 0.14904940128326416,
      "learning_rate": 5.236220472440945e-05,
      "loss": 0.14133179187774658,
      "step": 122
    },
    {
      "epoch": 0.9704142011834319,
      "grad_norm": 0.17290258407592773,
      "learning_rate": 5.1968503937007874e-05,
      "loss": 0.1813068389892578,
      "step": 123
    },
    {
      "epoch": 0.9783037475345168,
      "grad_norm": 0.14733834564685822,
      "learning_rate": 5.15748031496063e-05,
      "loss": 0.14911019802093506,
      "step": 124
    },
    {
      "epoch": 0.9861932938856016,
      "grad_norm": 0.18578411638736725,
      "learning_rate": 5.118110236220473e-05,
      "loss": 0.16401341557502747,
      "step": 125
    },
    {
      "epoch": 0.9940828402366864,
      "grad_norm": 0.17054852843284607,
      "learning_rate": 5.078740157480315e-05,
      "loss": 0.23275506496429443,
      "step": 126
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.1750422567129135,
      "learning_rate": 5.0393700787401575e-05,
      "loss": 0.17406979203224182,
      "step": 127
    },
    {
      "epoch": 1.0078895463510849,
      "grad_norm": 0.1386852115392685,
      "learning_rate": 5e-05,
      "loss": 0.1544576734304428,
      "step": 128
    },
    {
      "epoch": 1.0157790927021697,
      "grad_norm": 0.14368416368961334,
      "learning_rate": 4.960629921259843e-05,
      "loss": 0.11534670740365982,
      "step": 129
    },
    {
      "epoch": 1.0236686390532543,
      "grad_norm": 0.16433380544185638,
      "learning_rate": 4.9212598425196856e-05,
      "loss": 0.2346193492412567,
      "step": 130
    },
    {
      "epoch": 1.0315581854043392,
      "grad_norm": 0.1447460651397705,
      "learning_rate": 4.881889763779528e-05,
      "loss": 0.1431029736995697,
      "step": 131
    },
    {
      "epoch": 1.039447731755424,
      "grad_norm": 0.15651220083236694,
      "learning_rate": 4.84251968503937e-05,
      "loss": 0.19286054372787476,
      "step": 132
    },
    {
      "epoch": 1.047337278106509,
      "grad_norm": 0.1417357176542282,
      "learning_rate": 4.8031496062992124e-05,
      "loss": 0.1363014429807663,
      "step": 133
    },
    {
      "epoch": 1.0552268244575937,
      "grad_norm": 0.15485551953315735,
      "learning_rate": 4.763779527559055e-05,
      "loss": 0.48554033041000366,
      "step": 134
    },
    {
      "epoch": 1.0631163708086786,
      "grad_norm": 0.17385746538639069,
      "learning_rate": 4.724409448818898e-05,
      "loss": 0.1423502266407013,
      "step": 135
    },
    {
      "epoch": 1.0710059171597632,
      "grad_norm": 0.19883504509925842,
      "learning_rate": 4.6850393700787405e-05,
      "loss": 0.23389023542404175,
      "step": 136
    },
    {
      "epoch": 1.078895463510848,
      "grad_norm": 0.1603170484304428,
      "learning_rate": 4.645669291338583e-05,
      "loss": 0.17462590336799622,
      "step": 137
    },
    {
      "epoch": 1.086785009861933,
      "grad_norm": 0.18430450558662415,
      "learning_rate": 4.606299212598425e-05,
      "loss": 0.22346898913383484,
      "step": 138
    },
    {
      "epoch": 1.0946745562130178,
      "grad_norm": 0.18070435523986816,
      "learning_rate": 4.566929133858268e-05,
      "loss": 0.16581806540489197,
      "step": 139
    },
    {
      "epoch": 1.1025641025641026,
      "grad_norm": 0.17217859625816345,
      "learning_rate": 4.52755905511811e-05,
      "loss": 0.18443605303764343,
      "step": 140
    },
    {
      "epoch": 1.1104536489151873,
      "grad_norm": 0.17247027158737183,
      "learning_rate": 4.488188976377953e-05,
      "loss": 0.1462237536907196,
      "step": 141
    },
    {
      "epoch": 1.1183431952662721,
      "grad_norm": 0.17764881253242493,
      "learning_rate": 4.4488188976377954e-05,
      "loss": 0.15485522150993347,
      "step": 142
    },
    {
      "epoch": 1.126232741617357,
      "grad_norm": 0.22182093560695648,
      "learning_rate": 4.409448818897638e-05,
      "loss": 0.1939617097377777,
      "step": 143
    },
    {
      "epoch": 1.1341222879684418,
      "grad_norm": 0.15932142734527588,
      "learning_rate": 4.370078740157481e-05,
      "loss": 0.1415894627571106,
      "step": 144
    },
    {
      "epoch": 1.1420118343195267,
      "grad_norm": 0.15746566653251648,
      "learning_rate": 4.330708661417323e-05,
      "loss": 0.5883350968360901,
      "step": 145
    },
    {
      "epoch": 1.1499013806706113,
      "grad_norm": 0.23239852488040924,
      "learning_rate": 4.2913385826771655e-05,
      "loss": 0.1398998349905014,
      "step": 146
    },
    {
      "epoch": 1.1577909270216962,
      "grad_norm": 0.21518445014953613,
      "learning_rate": 4.251968503937008e-05,
      "loss": 0.20904363691806793,
      "step": 147
    },
    {
      "epoch": 1.165680473372781,
      "grad_norm": 0.21613742411136627,
      "learning_rate": 4.21259842519685e-05,
      "loss": 0.13916216790676117,
      "step": 148
    },
    {
      "epoch": 1.1735700197238659,
      "grad_norm": 0.18651458621025085,
      "learning_rate": 4.173228346456693e-05,
      "loss": 0.17994128167629242,
      "step": 149
    },
    {
      "epoch": 1.1814595660749507,
      "grad_norm": 0.16397421061992645,
      "learning_rate": 4.133858267716536e-05,
      "loss": 0.17714360356330872,
      "step": 150
    },
    {
      "epoch": 1.1893491124260356,
      "grad_norm": 0.19285017251968384,
      "learning_rate": 4.0944881889763784e-05,
      "loss": 0.19331765174865723,
      "step": 151
    },
    {
      "epoch": 1.1972386587771204,
      "grad_norm": 0.1664363294839859,
      "learning_rate": 4.0551181102362204e-05,
      "loss": 0.16332803666591644,
      "step": 152
    },
    {
      "epoch": 1.205128205128205,
      "grad_norm": 0.21620973944664001,
      "learning_rate": 4.015748031496063e-05,
      "loss": 0.1763688325881958,
      "step": 153
    },
    {
      "epoch": 1.21301775147929,
      "grad_norm": 0.17335747182369232,
      "learning_rate": 3.976377952755906e-05,
      "loss": 0.1724965125322342,
      "step": 154
    },
    {
      "epoch": 1.2209072978303748,
      "grad_norm": 0.16912506520748138,
      "learning_rate": 3.9370078740157485e-05,
      "loss": 0.21176058053970337,
      "step": 155
    },
    {
      "epoch": 1.2287968441814596,
      "grad_norm": 0.18002285063266754,
      "learning_rate": 3.8976377952755905e-05,
      "loss": 0.6262293457984924,
      "step": 156
    },
    {
      "epoch": 1.2366863905325443,
      "grad_norm": 0.1334623396396637,
      "learning_rate": 3.858267716535433e-05,
      "loss": 0.13240058720111847,
      "step": 157
    },
    {
      "epoch": 1.244575936883629,
      "grad_norm": 0.16430875658988953,
      "learning_rate": 3.818897637795276e-05,
      "loss": 0.14635376632213593,
      "step": 158
    },
    {
      "epoch": 1.252465483234714,
      "grad_norm": 0.1748051941394806,
      "learning_rate": 3.779527559055118e-05,
      "loss": 0.12602275609970093,
      "step": 159
    },
    {
      "epoch": 1.2603550295857988,
      "grad_norm": 0.1819901317358017,
      "learning_rate": 3.740157480314961e-05,
      "loss": 0.18371574580669403,
      "step": 160
    },
    {
      "epoch": 1.2682445759368837,
      "grad_norm": 0.147431880235672,
      "learning_rate": 3.7007874015748034e-05,
      "loss": 0.15703141689300537,
      "step": 161
    },
    {
      "epoch": 1.2761341222879685,
      "grad_norm": 0.1795821189880371,
      "learning_rate": 3.661417322834646e-05,
      "loss": 0.19155433773994446,
      "step": 162
    },
    {
      "epoch": 1.2840236686390534,
      "grad_norm": 0.16795597970485687,
      "learning_rate": 3.622047244094489e-05,
      "loss": 0.1707155406475067,
      "step": 163
    },
    {
      "epoch": 1.291913214990138,
      "grad_norm": 0.17486988008022308,
      "learning_rate": 3.582677165354331e-05,
      "loss": 0.1662619560956955,
      "step": 164
    },
    {
      "epoch": 1.2998027613412229,
      "grad_norm": 0.1635802537202835,
      "learning_rate": 3.5433070866141735e-05,
      "loss": 0.17407122254371643,
      "step": 165
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 0.15877285599708557,
      "learning_rate": 3.5039370078740156e-05,
      "loss": 0.13374972343444824,
      "step": 166
    },
    {
      "epoch": 1.3155818540433926,
      "grad_norm": 0.192097008228302,
      "learning_rate": 3.464566929133858e-05,
      "loss": 0.14126721024513245,
      "step": 167
    },
    {
      "epoch": 1.3234714003944772,
      "grad_norm": 0.20164577662944794,
      "learning_rate": 3.425196850393701e-05,
      "loss": 0.23616467416286469,
      "step": 168
    },
    {
      "epoch": 1.331360946745562,
      "grad_norm": 0.1795700192451477,
      "learning_rate": 3.385826771653544e-05,
      "loss": 0.1550510823726654,
      "step": 169
    },
    {
      "epoch": 1.339250493096647,
      "grad_norm": 0.1804218739271164,
      "learning_rate": 3.3464566929133864e-05,
      "loss": 0.2967161238193512,
      "step": 170
    },
    {
      "epoch": 1.3471400394477318,
      "grad_norm": 0.1596275120973587,
      "learning_rate": 3.3070866141732284e-05,
      "loss": 0.1426037698984146,
      "step": 171
    },
    {
      "epoch": 1.3550295857988166,
      "grad_norm": 0.19786246120929718,
      "learning_rate": 3.2677165354330704e-05,
      "loss": 0.18363535404205322,
      "step": 172
    },
    {
      "epoch": 1.3629191321499015,
      "grad_norm": 0.18387755751609802,
      "learning_rate": 3.228346456692913e-05,
      "loss": 0.1617846041917801,
      "step": 173
    },
    {
      "epoch": 1.3708086785009863,
      "grad_norm": 0.17420384287834167,
      "learning_rate": 3.188976377952756e-05,
      "loss": 0.2127019315958023,
      "step": 174
    },
    {
      "epoch": 1.378698224852071,
      "grad_norm": 0.21229633688926697,
      "learning_rate": 3.1496062992125985e-05,
      "loss": 0.18311280012130737,
      "step": 175
    },
    {
      "epoch": 1.3865877712031558,
      "grad_norm": 0.17352740466594696,
      "learning_rate": 3.110236220472441e-05,
      "loss": 0.1245657354593277,
      "step": 176
    },
    {
      "epoch": 1.3944773175542406,
      "grad_norm": 0.15263251960277557,
      "learning_rate": 3.070866141732284e-05,
      "loss": 0.3164919912815094,
      "step": 177
    },
    {
      "epoch": 1.4023668639053255,
      "grad_norm": 0.14680610597133636,
      "learning_rate": 3.0314960629921263e-05,
      "loss": 0.13949307799339294,
      "step": 178
    },
    {
      "epoch": 1.4102564102564101,
      "grad_norm": 0.18859495222568512,
      "learning_rate": 2.992125984251969e-05,
      "loss": 0.21241457760334015,
      "step": 179
    },
    {
      "epoch": 1.418145956607495,
      "grad_norm": 0.15768077969551086,
      "learning_rate": 2.952755905511811e-05,
      "loss": 0.402227520942688,
      "step": 180
    },
    {
      "epoch": 1.4260355029585798,
      "grad_norm": 0.14014342427253723,
      "learning_rate": 2.9133858267716534e-05,
      "loss": 0.1053699404001236,
      "step": 181
    },
    {
      "epoch": 1.4339250493096647,
      "grad_norm": 0.16216956079006195,
      "learning_rate": 2.874015748031496e-05,
      "loss": 0.1387379765510559,
      "step": 182
    },
    {
      "epoch": 1.4418145956607495,
      "grad_norm": 0.18212158977985382,
      "learning_rate": 2.8346456692913388e-05,
      "loss": 0.1450725942850113,
      "step": 183
    },
    {
      "epoch": 1.4497041420118344,
      "grad_norm": 0.24089206755161285,
      "learning_rate": 2.7952755905511812e-05,
      "loss": 0.1465035378932953,
      "step": 184
    },
    {
      "epoch": 1.4575936883629192,
      "grad_norm": 0.1303010731935501,
      "learning_rate": 2.755905511811024e-05,
      "loss": 0.10071345418691635,
      "step": 185
    },
    {
      "epoch": 1.4654832347140039,
      "grad_norm": 0.198194220662117,
      "learning_rate": 2.7165354330708666e-05,
      "loss": 0.20560358464717865,
      "step": 186
    },
    {
      "epoch": 1.4733727810650887,
      "grad_norm": 0.23849980533123016,
      "learning_rate": 2.677165354330709e-05,
      "loss": 0.21882297098636627,
      "step": 187
    },
    {
      "epoch": 1.4812623274161736,
      "grad_norm": 0.1827259361743927,
      "learning_rate": 2.637795275590551e-05,
      "loss": 0.26603615283966064,
      "step": 188
    },
    {
      "epoch": 1.4891518737672584,
      "grad_norm": 0.1762203872203827,
      "learning_rate": 2.5984251968503937e-05,
      "loss": 0.14797602593898773,
      "step": 189
    },
    {
      "epoch": 1.497041420118343,
      "grad_norm": 0.18490101397037506,
      "learning_rate": 2.5590551181102364e-05,
      "loss": 0.20967678725719452,
      "step": 190
    },
    {
      "epoch": 1.504930966469428,
      "grad_norm": 0.16980575025081635,
      "learning_rate": 2.5196850393700788e-05,
      "loss": 0.1330152153968811,
      "step": 191
    },
    {
      "epoch": 1.5128205128205128,
      "grad_norm": 0.15748998522758484,
      "learning_rate": 2.4803149606299215e-05,
      "loss": 0.14879460632801056,
      "step": 192
    },
    {
      "epoch": 1.5207100591715976,
      "grad_norm": 0.18784338235855103,
      "learning_rate": 2.440944881889764e-05,
      "loss": 0.20594780147075653,
      "step": 193
    },
    {
      "epoch": 1.5285996055226825,
      "grad_norm": 0.1859687864780426,
      "learning_rate": 2.4015748031496062e-05,
      "loss": 0.1662023961544037,
      "step": 194
    },
    {
      "epoch": 1.5364891518737673,
      "grad_norm": 0.18734288215637207,
      "learning_rate": 2.362204724409449e-05,
      "loss": 0.20349310338497162,
      "step": 195
    },
    {
      "epoch": 1.5443786982248522,
      "grad_norm": 0.1638457030057907,
      "learning_rate": 2.3228346456692916e-05,
      "loss": 0.13647069036960602,
      "step": 196
    },
    {
      "epoch": 1.552268244575937,
      "grad_norm": 0.18416020274162292,
      "learning_rate": 2.283464566929134e-05,
      "loss": 0.16804204881191254,
      "step": 197
    },
    {
      "epoch": 1.5601577909270217,
      "grad_norm": 0.15632940828800201,
      "learning_rate": 2.2440944881889763e-05,
      "loss": 0.15726135671138763,
      "step": 198
    },
    {
      "epoch": 1.5680473372781065,
      "grad_norm": 0.17438235878944397,
      "learning_rate": 2.204724409448819e-05,
      "loss": 0.21673886477947235,
      "step": 199
    },
    {
      "epoch": 1.5759368836291914,
      "grad_norm": 0.15484164655208588,
      "learning_rate": 2.1653543307086614e-05,
      "loss": 0.14272020757198334,
      "step": 200
    },
    {
      "epoch": 1.5759368836291914,
      "eval_loss": 0.1744702309370041,
      "eval_runtime": 45.5639,
      "eval_samples_per_second": 1.185,
      "eval_steps_per_second": 0.154,
      "step": 200
    },
    {
      "epoch": 1.583826429980276,
      "grad_norm": 0.14112474024295807,
      "learning_rate": 2.125984251968504e-05,
      "loss": 0.14459377527236938,
      "step": 201
    },
    {
      "epoch": 1.5917159763313609,
      "grad_norm": 0.15672320127487183,
      "learning_rate": 2.0866141732283465e-05,
      "loss": 0.14853420853614807,
      "step": 202
    },
    {
      "epoch": 1.5996055226824457,
      "grad_norm": 0.1666954606771469,
      "learning_rate": 2.0472440944881892e-05,
      "loss": 0.1748391091823578,
      "step": 203
    },
    {
      "epoch": 1.6074950690335306,
      "grad_norm": 0.1631515920162201,
      "learning_rate": 2.0078740157480316e-05,
      "loss": 0.15823905169963837,
      "step": 204
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 0.15599438548088074,
      "learning_rate": 1.9685039370078743e-05,
      "loss": 0.6035633683204651,
      "step": 205
    },
    {
      "epoch": 1.6232741617357003,
      "grad_norm": 0.17157207429409027,
      "learning_rate": 1.9291338582677166e-05,
      "loss": 0.1618078500032425,
      "step": 206
    },
    {
      "epoch": 1.6311637080867851,
      "grad_norm": 0.19649115204811096,
      "learning_rate": 1.889763779527559e-05,
      "loss": 0.22818244993686676,
      "step": 207
    },
    {
      "epoch": 1.63905325443787,
      "grad_norm": 0.1489448845386505,
      "learning_rate": 1.8503937007874017e-05,
      "loss": 0.13948802649974823,
      "step": 208
    },
    {
      "epoch": 1.6469428007889546,
      "grad_norm": 0.1630081683397293,
      "learning_rate": 1.8110236220472444e-05,
      "loss": 0.12975259125232697,
      "step": 209
    },
    {
      "epoch": 1.6548323471400395,
      "grad_norm": 0.18848545849323273,
      "learning_rate": 1.7716535433070868e-05,
      "loss": 0.18708962202072144,
      "step": 210
    },
    {
      "epoch": 1.6627218934911243,
      "grad_norm": 0.18682488799095154,
      "learning_rate": 1.732283464566929e-05,
      "loss": 0.17205756902694702,
      "step": 211
    },
    {
      "epoch": 1.670611439842209,
      "grad_norm": 0.15827827155590057,
      "learning_rate": 1.692913385826772e-05,
      "loss": 0.09878293424844742,
      "step": 212
    },
    {
      "epoch": 1.6785009861932938,
      "grad_norm": 0.1453385204076767,
      "learning_rate": 1.6535433070866142e-05,
      "loss": 0.1378372758626938,
      "step": 213
    },
    {
      "epoch": 1.6863905325443787,
      "grad_norm": 0.19114550948143005,
      "learning_rate": 1.6141732283464566e-05,
      "loss": 0.1819063276052475,
      "step": 214
    },
    {
      "epoch": 1.6942800788954635,
      "grad_norm": 0.1731165200471878,
      "learning_rate": 1.5748031496062993e-05,
      "loss": 0.18171846866607666,
      "step": 215
    },
    {
      "epoch": 1.7021696252465484,
      "grad_norm": 0.15948548913002014,
      "learning_rate": 1.535433070866142e-05,
      "loss": 0.15313774347305298,
      "step": 216
    },
    {
      "epoch": 1.7100591715976332,
      "grad_norm": 0.1429692506790161,
      "learning_rate": 1.4960629921259845e-05,
      "loss": 0.09686063230037689,
      "step": 217
    },
    {
      "epoch": 1.717948717948718,
      "grad_norm": 0.16163882613182068,
      "learning_rate": 1.4566929133858267e-05,
      "loss": 0.1804504543542862,
      "step": 218
    },
    {
      "epoch": 1.725838264299803,
      "grad_norm": 0.1702345311641693,
      "learning_rate": 1.4173228346456694e-05,
      "loss": 0.19020865857601166,
      "step": 219
    },
    {
      "epoch": 1.7337278106508875,
      "grad_norm": 0.1463574469089508,
      "learning_rate": 1.377952755905512e-05,
      "loss": 0.12233975529670715,
      "step": 220
    },
    {
      "epoch": 1.7416173570019724,
      "grad_norm": 0.26056936383247375,
      "learning_rate": 1.3385826771653545e-05,
      "loss": 0.18920640647411346,
      "step": 221
    },
    {
      "epoch": 1.7495069033530573,
      "grad_norm": 0.20972047746181488,
      "learning_rate": 1.2992125984251968e-05,
      "loss": 0.2367677390575409,
      "step": 222
    },
    {
      "epoch": 1.7573964497041419,
      "grad_norm": 0.15191878378391266,
      "learning_rate": 1.2598425196850394e-05,
      "loss": 0.1127471849322319,
      "step": 223
    },
    {
      "epoch": 1.7652859960552267,
      "grad_norm": 0.23475293815135956,
      "learning_rate": 1.220472440944882e-05,
      "loss": 0.14210370182991028,
      "step": 224
    },
    {
      "epoch": 1.7731755424063116,
      "grad_norm": 0.1783781200647354,
      "learning_rate": 1.1811023622047245e-05,
      "loss": 0.20386657118797302,
      "step": 225
    },
    {
      "epoch": 1.7810650887573964,
      "grad_norm": 0.17878372967243195,
      "learning_rate": 1.141732283464567e-05,
      "loss": 0.15251412987709045,
      "step": 226
    },
    {
      "epoch": 1.7889546351084813,
      "grad_norm": 0.18270187079906464,
      "learning_rate": 1.1023622047244095e-05,
      "loss": 0.20618554949760437,
      "step": 227
    },
    {
      "epoch": 1.7968441814595661,
      "grad_norm": 0.19168847799301147,
      "learning_rate": 1.062992125984252e-05,
      "loss": 0.22534866631031036,
      "step": 228
    },
    {
      "epoch": 1.804733727810651,
      "grad_norm": 0.20888398587703705,
      "learning_rate": 1.0236220472440946e-05,
      "loss": 0.15195992588996887,
      "step": 229
    },
    {
      "epoch": 1.8126232741617359,
      "grad_norm": 0.24502836167812347,
      "learning_rate": 9.842519685039371e-06,
      "loss": 0.20719824731349945,
      "step": 230
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.19252359867095947,
      "learning_rate": 9.448818897637795e-06,
      "loss": 0.191138356924057,
      "step": 231
    },
    {
      "epoch": 1.8284023668639053,
      "grad_norm": 0.1668490320444107,
      "learning_rate": 9.055118110236222e-06,
      "loss": 0.2707330584526062,
      "step": 232
    },
    {
      "epoch": 1.8362919132149902,
      "grad_norm": 0.1626531481742859,
      "learning_rate": 8.661417322834646e-06,
      "loss": 0.12498346716165543,
      "step": 233
    },
    {
      "epoch": 1.8441814595660748,
      "grad_norm": 0.17463310062885284,
      "learning_rate": 8.267716535433071e-06,
      "loss": 0.13782939314842224,
      "step": 234
    },
    {
      "epoch": 1.8520710059171597,
      "grad_norm": 0.15452773869037628,
      "learning_rate": 7.874015748031496e-06,
      "loss": 0.13151168823242188,
      "step": 235
    },
    {
      "epoch": 1.8599605522682445,
      "grad_norm": 0.20611026883125305,
      "learning_rate": 7.4803149606299226e-06,
      "loss": 0.200051411986351,
      "step": 236
    },
    {
      "epoch": 1.8678500986193294,
      "grad_norm": 0.17364627122879028,
      "learning_rate": 7.086614173228347e-06,
      "loss": 0.16379345953464508,
      "step": 237
    },
    {
      "epoch": 1.8757396449704142,
      "grad_norm": 0.22457212209701538,
      "learning_rate": 6.692913385826772e-06,
      "loss": 0.18649311363697052,
      "step": 238
    },
    {
      "epoch": 1.883629191321499,
      "grad_norm": 0.1645238846540451,
      "learning_rate": 6.299212598425197e-06,
      "loss": 0.17010004818439484,
      "step": 239
    },
    {
      "epoch": 1.891518737672584,
      "grad_norm": 0.16262532770633698,
      "learning_rate": 5.905511811023622e-06,
      "loss": 0.17134612798690796,
      "step": 240
    },
    {
      "epoch": 1.8994082840236688,
      "grad_norm": 0.18261636793613434,
      "learning_rate": 5.511811023622048e-06,
      "loss": 0.20325730741024017,
      "step": 241
    },
    {
      "epoch": 1.9072978303747534,
      "grad_norm": 0.19294697046279907,
      "learning_rate": 5.118110236220473e-06,
      "loss": 0.17491135001182556,
      "step": 242
    },
    {
      "epoch": 1.9151873767258383,
      "grad_norm": 0.16230672597885132,
      "learning_rate": 4.7244094488188975e-06,
      "loss": 0.15936928987503052,
      "step": 243
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 0.1708834320306778,
      "learning_rate": 4.330708661417323e-06,
      "loss": 0.24394884705543518,
      "step": 244
    },
    {
      "epoch": 1.9309664694280078,
      "grad_norm": 0.18153634667396545,
      "learning_rate": 3.937007874015748e-06,
      "loss": 0.17409631609916687,
      "step": 245
    },
    {
      "epoch": 1.9388560157790926,
      "grad_norm": 0.2531120479106903,
      "learning_rate": 3.5433070866141735e-06,
      "loss": 0.16805899143218994,
      "step": 246
    },
    {
      "epoch": 1.9467455621301775,
      "grad_norm": 0.1583506017923355,
      "learning_rate": 3.1496062992125985e-06,
      "loss": 0.15417180955410004,
      "step": 247
    },
    {
      "epoch": 1.9546351084812623,
      "grad_norm": 0.14028804004192352,
      "learning_rate": 2.755905511811024e-06,
      "loss": 0.14463350176811218,
      "step": 248
    },
    {
      "epoch": 1.9625246548323472,
      "grad_norm": 0.1788078248500824,
      "learning_rate": 2.3622047244094487e-06,
      "loss": 0.21345160901546478,
      "step": 249
    },
    {
      "epoch": 1.970414201183432,
      "grad_norm": 0.17253437638282776,
      "learning_rate": 1.968503937007874e-06,
      "loss": 0.16974912583827972,
      "step": 250
    },
    {
      "epoch": 1.9783037475345169,
      "grad_norm": 0.19813571870326996,
      "learning_rate": 1.5748031496062992e-06,
      "loss": 0.23829026520252228,
      "step": 251
    },
    {
      "epoch": 1.9861932938856017,
      "grad_norm": 0.1470332145690918,
      "learning_rate": 1.1811023622047244e-06,
      "loss": 0.13210810720920563,
      "step": 252
    },
    {
      "epoch": 1.9940828402366864,
      "grad_norm": 0.15620601177215576,
      "learning_rate": 7.874015748031496e-07,
      "loss": 0.13364121317863464,
      "step": 253
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.19811704754829407,
      "learning_rate": 3.937007874015748e-07,
      "loss": 0.15226100385189056,
      "step": 254
    }
  ],
  "logging_steps": 1,
  "max_steps": 254,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 3,
        "early_stopping_threshold": 0.0001
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.659687847878533e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
